<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>HD Voice Bot (Browser)</title>
  <style>
    body { font-family: system-ui, Arial; margin: 20px; }
    button { padding: 10px 16px; margin-right: 8px; }
    #log { margin-top: 12px; white-space: pre-wrap; background:#f6f6f8;padding:10px;border-radius:6px;max-height:300px;overflow:auto;}
    input { padding:6px; }
  </style>
</head>
<body>
  <h2>HD Voice Bot (Browser)</h2>
  <p>Click Start → Speak → Stop. Captures audio, downsamples to 16k PCM, sends raw Int16 PCM binary frames to WS (~320ms chunks). Server throttles with control messages.</p>

  <button id="startBtn">Start</button>
  <button id="stopBtn" disabled>Stop</button>
  <div>
    <label>Server WS URL: <input id="wsUrl" style="width:420px" value="wss://botcore-z6j0.onrender.com/ws/hd-audio?token=mysecret123"/></label>
  </div>
  <div id="status">Status: Idle</div>
  <div id="log"></div>

<script>
const startBtn = document.getElementById('startBtn');
const stopBtn  = document.getElementById('stopBtn');
const statusEl = document.getElementById('status');
const logEl    = document.getElementById('log');
const wsInput  = document.getElementById('wsUrl');

let ws = null;
let micStream = null;
let playbackCtx = null;
let paused = false;          // client-side pause due to server throttle
let serverThrottled = false; // server told to throttle
let bufferedPause = false;   // local backpressure fallback

// CONFIG
const TARGET_SR = 16000;            // server STT sample rate
const CHUNK_MS = 320;               // send ~320ms frames
const MAX_BUFFERED_AMOUNT = 1_500_000;  // bytes: pause sending when ws.bufferedAmount exceeds this
const RESUME_BUFFERED_AMOUNT = 600_000; // resume threshold

function log(msg) {
  console.log(msg);
  logEl.textContent += msg + '\n';
  logEl.scrollTop = logEl.scrollHeight;
}

async function ensurePlaybackContext() {
  if (!playbackCtx) {
    playbackCtx = new (window.AudioContext || window.webkitAudioContext)();
    try { await playbackCtx.resume(); } catch(e) {}
  }
  return playbackCtx;
}

async function handleRemoteAudioBase64(base64Wav) {
  try {
    const ab = Uint8Array.from(atob(base64Wav), c => c.charCodeAt(0)).buffer;
    const ctx = await ensurePlaybackContext();
    const decoded = await ctx.decodeAudioData(ab.slice(0)); // copy for some browsers
    const src = ctx.createBufferSource();
    src.buffer = decoded;
    src.connect(ctx.destination);
    src.start(0);
    log('Playing remote audio (bytes=' + ab.byteLength + ')');
  } catch (e) {
    log('Error playing remote audio: ' + e);
  }
}

// naive downsample (linear pick) - fast and deterministic
function downsampleBuffer(buffer, inputRate, outputRate) {
  if (outputRate === inputRate) {
    return buffer;
  }
  const sampleRatio = inputRate / outputRate;
  const outLength = Math.floor(buffer.length / sampleRatio);
  const out = new Float32Array(outLength);
  let offset = 0;
  for (let i = 0; i < outLength; i++, offset += sampleRatio) {
    out[i] = buffer[Math.floor(offset)] || 0;
  }
  return out;
}

// convert Float32 -> Int16 ArrayBuffer
function floatToInt16Buffer(float32Arr) {
  const int16 = new Int16Array(float32Arr.length);
  for (let i = 0; i < float32Arr.length; i++) {
    let s = Math.max(-1, Math.min(1, float32Arr[i]));
    int16[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
  }
  return int16.buffer;
}

async function startStreaming() {
  const wsUrl = wsInput.value.trim();
  if (!wsUrl) { log('Missing WS URL'); return; }

  ws = new WebSocket(wsUrl);
  ws.binaryType = "arraybuffer";

  ws.onopen = () => {
    statusEl.textContent = 'Status: Connected to server';
    log('WS connected');
    // send control start as text JSON
    ws.send(JSON.stringify({type:'start', meta:{client:'web-client'}}));
  };

  ws.onerror = (e) => {
    log('WS error: ' + e);
    statusEl.textContent = 'Status: WS error';
  };

  ws.onclose = () => {
    log('WS closed');
    statusEl.textContent = 'Status: Disconnected';
  };

  ws.onmessage = (ev) => {
    try {
      const txt = typeof ev.data === 'string' ? ev.data : null;
      if (txt) {
        const msg = JSON.parse(txt);
        if (msg.type === 'transcript') {
          log('[TRANSCRIPT] ' + msg.text);
        } else if (msg.type === 'ai_text') {
          log('[AI] ' + msg.text);
        } else if (msg.type === 'audio') {
          handleRemoteAudioBase64(msg.audio);
        } else if (msg.type === 'control') {
          if (msg.action === 'stop_playback') {
            log('[SERVER CONTROL] stop_playback');
          } else if (msg.action === 'throttle') {
            serverThrottled = true;
            paused = true;
            log('[SERVER CONTROL] throttle — pausing sends');
          } else if (msg.action === 'resume') {
            serverThrottled = false;
            paused = false;
            log('[SERVER CONTROL] resume — resuming sends');
          }
        } else if (msg.type === 'error') {
          log('[SERVER ERROR] ' + msg.error);
        } else {
          log('[WS] ' + JSON.stringify(msg).slice(0,200));
        }
      } else {
        log('[WS] Received binary message length=' + (ev.data && ev.data.byteLength ? ev.data.byteLength : 0));
      }
    } catch (e) {
      log('WS message parse error: ' + e);
    }
  };

  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    micStream = stream;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    await audioCtx.resume();

    const source = audioCtx.createMediaStreamSource(stream);

    // ScriptProcessorNode (compat). We accumulate to CHUNK_MS sized frames at TARGET_SR.
    const processor = audioCtx.createScriptProcessor(4096, 1, 1);

    let acc = [];
    const inputRate = audioCtx.sampleRate || 48000;
    const samplesPerChunk = Math.ceil((CHUNK_MS / 1000) * TARGET_SR);

    processor.onaudioprocess = (e) => {
      if (!ws || ws.readyState !== WebSocket.OPEN) return;
      if (paused) return; // honor server throttle or local pause

      const input = e.inputBuffer.getChannelData(0); // Float32Array
      const down = downsampleBuffer(input, inputRate, TARGET_SR);
      for (let i = 0; i < down.length; i++) acc.push(down[i]);

      while (acc.length >= samplesPerChunk) {
        const chunkSamples = acc.splice(0, samplesPerChunk);
        const ab = floatToInt16Buffer(chunkSamples);
        // client-local backpressure: prefer server throttle, but also watch bufferedAmount
        if (ws.bufferedAmount > MAX_BUFFERED_AMOUNT) {
          // pause locally until buffered reduces
          bufferedPause = true;
          paused = true;
          log('Local pause: ws.bufferedAmount=' + ws.bufferedAmount);
          const resumeCheck = setInterval(() => {
            if (!ws || ws.readyState !== WebSocket.OPEN) {
              clearInterval(resumeCheck);
              bufferedPause = false;
              paused = false;
              return;
            }
            if (ws.bufferedAmount <= RESUME_BUFFERED_AMOUNT && !serverThrottled) {
              clearInterval(resumeCheck);
              bufferedPause = false;
              paused = false;
              log('Local resume: bufferedAmount=' + ws.bufferedAmount);
            }
          }, 120);
        }

        if (!paused) {
          try {
            ws.send(ab); // send raw ArrayBuffer binary frame
            log('Sent chunk bytes=' + (ab.byteLength) + ' samples=' + (ab.byteLength/2) + ' buffered=' + ws.bufferedAmount);
          } catch (e) {
            console.error('WS send audio failed', e);
          }
        } else {
          // if paused, put samples back front and break loop
          acc = chunkSamples.concat(acc);
          break;
        }
      }
    };

    source.connect(processor);
    // connect processor to destination to keep the node alive in some browsers
    processor.connect(audioCtx.destination);

    window._hdAudio = { source, processor, audioCtx, stream };

    statusEl.textContent = 'Status: Recording...';
    log('Recording started (batched -> 16k PCM, CHUNK_MS=' + CHUNK_MS + 'ms)');
    startBtn.disabled = true;
    stopBtn.disabled = false;
  } catch (e) {
    log('getUserMedia / audio setup error: ' + e);
  }
}

async function stopStreaming() {
  try {
    if (window._hdAudio) {
      const { source, processor, audioCtx } = window._hdAudio;
      try { source.disconnect(); } catch(e){}
      try { processor.disconnect(); } catch(e){}
      try { await audioCtx.close(); } catch(e){}
      window._hdAudio = null;
    }

    if (micStream) {
      micStream.getTracks().forEach(t => t.stop());
      micStream = null;
    }

    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({type:'stop'}));
      setTimeout(()=>{ try { ws.close(); } catch(e){} ws=null; }, 500);
    }

    statusEl.textContent = 'Status: Stopped';
    log('Recording stopped');
    startBtn.disabled = false;
    stopBtn.disabled = true;
    paused = false;
    serverThrottled = false;
    bufferedPause = false;
  } catch(e) {
    log('stop error: ' + e);
  }
}

startBtn.addEventListener('click', async () => {
  await startStreaming();
});

stopBtn.addEventListener('click', async () => {
  await stopStreaming();
});
</script>
</body>
</html>
