
<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>HD Voice Bot (Browser)</title>
  <style>
    body { font-family: system-ui, Arial; margin: 20px; }
    button { padding: 10px 16px; margin-right: 8px; }
    #log { margin-top: 12px; white-space: pre-wrap; background:#f6f6f8;padding:10px;border-radius:6px;max-height:300px;overflow:auto;}
  </style>
</head>
<body>
  <h2>HD Voice Bot (Browser)</h2>
  <p>Click Start → Speak → Stop. Captures audio, downsamples to 16k PCM, sends raw Int16 PCM binary frames to WS (320ms chunks). Uses throttling based on ws.bufferedAmount.</p>
  <button id="startBtn">Start</button>
  <button id="stopBtn" disabled>Stop</button>
  <div>
    <label>Server WS URL: <input id="wsUrl" style="width:420px" value="wss://botcore-z6j0.onrender.com/ws/hd-audio?token=mysecret123"/></label>
  </div>
  <div id="status">Status: Idle</div>
  <div id="log"></div>

<script>
const startBtn = document.getElementById('startBtn');
const stopBtn  = document.getElementById('stopBtn');
const statusEl = document.getElementById('status');
const logEl    = document.getElementById('log');
const wsInput  = document.getElementById('wsUrl');

let ws = null;
let micStream = null;
let playbackCtx = null;

// CONFIG
const TARGET_SR = 16000;            // server STT sample rate
const CHUNK_MS = 320;               // send ~320ms frames
const MAX_BUFFERED_AMOUNT = 1_000_000;  // bytes: pause sending when ws.bufferedAmount exceeds this
const RESUME_BUFFERED_AMOUNT = 400_000; // resume threshold

function log(msg) {
  console.log(msg);
  logEl.textContent += msg + '\n';
  logEl.scrollTop = logEl.scrollHeight;
}

async function ensurePlaybackContext() {
  if (!playbackCtx) {
    playbackCtx = new (window.AudioContext || window.webkitAudioContext)();
    try { await playbackCtx.resume(); } catch(e) {}
  }
  return playbackCtx;
}

async function handleRemoteAudioBase64(base64Wav) {
  try {
    const ab = Uint8Array.from(atob(base64Wav), c => c.charCodeAt(0)).buffer;
    const ctx = await ensurePlaybackContext();
    const decoded = await ctx.decodeAudioData(ab.slice(0)); // some browsers require copy
    const src = ctx.createBufferSource();
    src.buffer = decoded;
    src.connect(ctx.destination);
    src.start(0);
    log('Playing remote audio (bytes=' + ab.byteLength + ')');
  } catch (e) {
    log('Error playing remote audio: ' + e);
  }
}

function downsampleBuffer(buffer, inputRate, outputRate) {
  if (outputRate === inputRate) {
    return buffer;
  }
  const sampleRatio = inputRate / outputRate;
  const outLength = Math.floor(buffer.length / sampleRatio);
  const out = new Float32Array(outLength);
  let offset = 0;
  for (let i = 0; i < outLength; i++, offset += sampleRatio) {
    out[i] = buffer[Math.floor(offset)] || 0;
  }
  return out;
}

async function startStreaming() {
  const wsUrl = wsInput.value.trim();
  if (!wsUrl) { log('Missing WS URL'); return; }

  ws = new WebSocket(wsUrl);
  ws.binaryType = "arraybuffer";

  ws.onopen = () => {
    statusEl.textContent = 'Status: Connected to server';
    log('WS connected');
    // send control start as text JSON
    ws.send(JSON.stringify({type:'start', meta:{client:'web-client'}}));
  };

  ws.onerror = (e) => {
    log('WS error: ' + e);
    statusEl.textContent = 'Status: WS error';
  };

  ws.onclose = () => {
    log('WS closed');
    statusEl.textContent = 'Status: Disconnected';
  };

  ws.onmessage = (ev) => {
    try {
      // server sends textual JSON for control / ai_text / transcript / audio (audio base64 inside JSON)
      const txt = typeof ev.data === 'string' ? ev.data : null;
      if (txt) {
        const msg = JSON.parse(txt);
        if (msg.type === 'transcript') {
          log('[TRANSCRIPT] ' + msg.text);
        } else if (msg.type === 'ai_text') {
          log('[AI] ' + msg.text);
        } else if (msg.type === 'audio') {
          handleRemoteAudioBase64(msg.audio);
        } else if (msg.type === 'control' && msg.action === 'stop_playback') {
          log('[SERVER CONTROL] stop_playback');
        } else if (msg.type === 'error') {
          log('[SERVER ERROR] ' + msg.error);
        } else {
          log('[WS] ' + JSON.stringify(msg).slice(0,200));
        }
      } else {
        // binary message from server (unlikely here because server sends base64 in JSON), but handle if needed
        log('[WS] Received binary message length=' + (ev.data && ev.data.byteLength ? ev.data.byteLength : 0));
      }
    } catch (e) {
      log('WS message parse error: ' + e);
    }
  };

  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    micStream = stream;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    await audioCtx.resume();

    const source = audioCtx.createMediaStreamSource(stream);

    // We'll use ScriptProcessorNode for compatibility, but we will accumulate to larger send chunks.
    const processor = audioCtx.createScriptProcessor(4096, 1, 1);

    // Buffer for accumulation (Float32)
    let acc = [];
    const inputRate = audioCtx.sampleRate || 48000;
    const samplesPerChunk = Math.ceil((CHUNK_MS / 1000) * TARGET_SR); // target number of samples at 16k
    // Note: we'll downsample each onaudioprocess call and append

    let paused = false;

    processor.onaudioprocess = (e) => {
      if (!ws || ws.readyState !== WebSocket.OPEN) return;

      const input = e.inputBuffer.getChannelData(0); // Float32Array
      // downsample to target rate (float)
      const down = downsampleBuffer(input, inputRate, TARGET_SR); // Float32Array
      // append
      for (let i = 0; i < down.length; i++) acc.push(down[i]);

      // if we have enough samples to send a chunk
      while (acc.length >= samplesPerChunk) {
        const chunkSamples = acc.splice(0, samplesPerChunk);
        // convert Float32 -> Int16
        const int16 = new Int16Array(chunkSamples.length);
        for (let i = 0; i < chunkSamples.length; i++) {
          let s = Math.max(-1, Math.min(1, chunkSamples[i]));
          int16[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
        }
        const buffer = int16.buffer;

        // backpressure: pause sending if websocket bufferedAmount grows too large
        if (ws.bufferedAmount > MAX_BUFFERED_AMOUNT) {
          paused = true;
          log('Pausing sends; bufferedAmount=' + ws.bufferedAmount);
          setTimeout(() => {
            const checkResume = setInterval(() => {
              if (!ws || ws.readyState !== WebSocket.OPEN) {
                clearInterval(checkResume);
                paused = false;
                return;
              }
              if (ws.bufferedAmount <= RESUME_BUFFERED_AMOUNT) {
                paused = false;
                clearInterval(checkResume);
                log('Resuming sends; bufferedAmount=' + ws.bufferedAmount);
              }
            }, 120);
          }, 0);
        }

        if (!paused) {
          try {
            ws.send(buffer); // send raw ArrayBuffer (binary frame)
            log('Sent chunk bytes=' + (int16.length*2) + ' samples=' + int16.length + ' buffered=' + ws.bufferedAmount);
          } catch (e) {
            console.error('WS send audio failed', e);
          }
        } else {
          // if paused, re-insert chunk front so we don't lose
          acc = chunkSamples.concat(acc);
          break;
        }
      }
    };

    source.connect(processor);
    // connect processor to destination to keep it running in some browsers
    processor.connect(audioCtx.destination);

    window._hdAudio = { source, processor, audioCtx, stream };

    statusEl.textContent = 'Status: Recording...';
    log('Recording started (batched -> 16k PCM, CHUNK_MS=' + CHUNK_MS + 'ms)');
    startBtn.disabled = true;
    stopBtn.disabled = false;
  } catch (e) {
    log('getUserMedia / audio setup error: ' + e);
  }
}

async function stopStreaming() {
  try {
    if (window._hdAudio) {
      const { source, processor, audioCtx } = window._hdAudio;
      try { source.disconnect(); } catch(e){}
      try { processor.disconnect(); } catch(e){}
      try { await audioCtx.close(); } catch(e){}
      window._hdAudio = null;
    }

    if (micStream) {
      micStream.getTracks().forEach(t => t.stop());
      micStream = null;
    }

    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({type:'stop'}));
      setTimeout(()=>{ try { ws.close(); } catch(e){} ws=null; }, 500);
    }

    statusEl.textContent = 'Status: Stopped';
    log('Recording stopped');
    startBtn.disabled = false;
    stopBtn.disabled = true;
  } catch(e) {
    log('stop error: ' + e);
  }
}

startBtn.addEventListener('click', async () => {
  await startStreaming();
});

stopBtn.addEventListener('click', async () => {
  await stopStreaming();
});
</script>
</body>
</html>
