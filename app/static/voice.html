<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>HD Voice Bot (Browser)</title>
  <style>
    body { font-family: system-ui, Arial; margin: 20px; }
    button { padding: 10px 16px; margin-right: 8px; }
    #log { margin-top: 12px; white-space: pre-wrap; background:#f6f6f8;padding:10px;border-radius:6px;max-height:300px;overflow:auto;}
  </style>
</head>
<body>
  <h2>HD Voice Bot (Browser)</h2>
  <p>Click Start → Speak → Stop. Captures audio directly, downsamples to 16k PCM, sends to WS, plays HD TTS from server.</p>
  <button id="startBtn">Start</button>
  <button id="stopBtn" disabled>Stop</button>
  <div>
    <label>Server WS URL: <input id="wsUrl" style="width:420px" value="wss://botcore-z6j0.onrender.com/ws/hd-audio?token=mysecret123"/></label>
  </div>
  <div id="status">Status: Idle</div>
  <div id="log"></div>

<script>
/*
Direct-capture frontend:
 - Use getUserMedia -> AudioContext -> ScriptProcessorNode
 - Convert Float32 -> downsample -> Int16 PCM -> base64 -> send JSON {type:'audio', payload: base64}
 - Receive server WAV base64 and play via AudioContext
*/

const startBtn = document.getElementById('startBtn');
const stopBtn  = document.getElementById('stopBtn');
const statusEl = document.getElementById('status');
const logEl    = document.getElementById('log');
const wsInput  = document.getElementById('wsUrl');

let ws = null;
let micStream = null;
let playbackCtx = null;

function log(msg) {
  console.log(msg);
  logEl.textContent += msg + '\n';
  logEl.scrollTop = logEl.scrollHeight;
}

function arrayBufferToBase64(buffer){
  let binary = '';
  const bytes = new Uint8Array(buffer);
  const chunk = 0x8000;
  for (let i = 0; i < bytes.length; i += chunk) {
    binary += String.fromCharCode.apply(null, bytes.subarray(i, i + chunk));
  }
  return btoa(binary);
}

function base64ToArrayBuffer(base64){
  const binary = atob(base64);
  const len = binary.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) bytes[i] = binary.charCodeAt(i);
  return bytes.buffer;
}

async function ensurePlaybackContext() {
  if (!playbackCtx) {
    playbackCtx = new (window.AudioContext || window.webkitAudioContext)();
    try { await playbackCtx.resume(); } catch(e) {}
  }
  return playbackCtx;
}

async function handleRemoteAudioBase64(base64Wav) {
  try {
    const ab = base64ToArrayBuffer(base64Wav);
    const ctx = await ensurePlaybackContext();
    const decoded = await ctx.decodeAudioData(ab);
    const src = ctx.createBufferSource();
    src.buffer = decoded;
    src.connect(ctx.destination);
    src.start(0);
    log('Playing remote audio (bytes=' + ab.byteLength + ')');
  } catch (e) {
    log('Error playing remote audio: ' + e);
  }
}

async function startStreaming() {
  const wsUrl = wsInput.value.trim();
  if (!wsUrl) { log('Missing WS URL'); return; }

  ws = new WebSocket(wsUrl);
  ws.binaryType = "arraybuffer";

  ws.onopen = () => {
    statusEl.textContent = 'Status: Connected to server';
    log('WS connected');
    ws.send(JSON.stringify({type:'start', meta:{client:'web-client'}}));
  };

  ws.onerror = (e) => {
    log('WS error: ' + e);
    statusEl.textContent = 'Status: WS error';
  };

  ws.onclose = () => {
    log('WS closed');
    statusEl.textContent = 'Status: Disconnected';
  };

  ws.onmessage = (ev) => {
    try {
      const msg = JSON.parse(ev.data);
      if (msg.type === 'transcript') {
        log('[TRANSCRIPT] ' + msg.text);
      } else if (msg.type === 'ai_text') {
        log('[AI] ' + msg.text);
      } else if (msg.type === 'audio') {
        handleRemoteAudioBase64(msg.audio);
      } else if (msg.type === 'error') {
        log('[SERVER ERROR] ' + msg.error);
      } else {
        log('[WS] ' + JSON.stringify(msg).slice(0,200));
      }
    } catch (e) {
      log('WS message parse error: ' + e);
    }
  };

  // direct capture using ScriptProcessorNode (widely supported)
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    micStream = stream;

    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    await audioCtx.resume(); // CRITICAL for many browsers

    const source = audioCtx.createMediaStreamSource(stream);
    // buffer size 4096 is a good tradeoff; can tune to 2048/8192
    const processor = audioCtx.createScriptProcessor(4096, 1, 1);

    processor.onaudioprocess = (e) => {
      if (!ws || ws.readyState !== WebSocket.OPEN) return;

      // get Float32 mono input
      const input = e.inputBuffer.getChannelData(0);
      const inputRate = audioCtx.sampleRate || 48000;
      const targetRate = 16000;

      // simple downsample (nearest sample) when rates differ
      let out;
      if (inputRate === targetRate) {
        out = input;
      } else {
        const ratio = inputRate / targetRate;
        const outLen = Math.floor(input.length / ratio);
        out = new Float32Array(outLen);
        for (let i = 0, j = 0; j < outLen; j++, i += ratio) {
          out[j] = input[Math.floor(i)] || 0;
        }
      }

      // float32 -> int16 PCM little-endian
      const buffer = new ArrayBuffer(out.length * 2);
      const view = new DataView(buffer);
      for (let i = 0; i < out.length; i++) {
        let s = Math.max(-1, Math.min(1, out[i]));
        view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7fff, true);
      }

      // base64 and send
      const b64 = arrayBufferToBase64(buffer);
      try {
        ws.send(JSON.stringify({type:'audio', payload: b64}));
        log('Sent audio chunk bytes=' + buffer.byteLength + ' b64len=' + b64.length);
      } catch (e) {
        console.error('WS send audio failed', e);
      }
    };

    source.connect(processor);
    processor.connect(audioCtx.destination); // needed on some browsers to keep processor running

    window._hdAudio = { source, processor, audioCtx, stream };

    statusEl.textContent = 'Status: Recording...';
    log('Recording started (direct capture -> 16k PCM)');
    startBtn.disabled = true;
    stopBtn.disabled = false;
  } catch (e) {
    log('getUserMedia / audio setup error: ' + e);
  }
}

async function stopStreaming() {
  try {
    // stop direct-capture nodes
    if (window._hdAudio) {
      const { source, processor, audioCtx } = window._hdAudio;
      try { source.disconnect(); } catch(e){}
      try { processor.disconnect(); } catch(e){}
      try { await audioCtx.close(); } catch(e){}
      window._hdAudio = null;
    }

    if (micStream) {
      micStream.getTracks().forEach(t => t.stop());
      micStream = null;
    }

    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({type:'stop'}));
      // allow server to finalize
      setTimeout(()=>{ try { ws.close(); } catch(e){} ws=null; }, 500);
    }

    statusEl.textContent = 'Status: Stopped';
    log('Recording stopped');
    startBtn.disabled = false;
    stopBtn.disabled = true;
  } catch(e) {
    log('stop error: ' + e);
  }
}

startBtn.addEventListener('click', async () => {
  await startStreaming();
});

stopBtn.addEventListener('click', async () => {
  await stopStreaming();
});
</script>
</body>
</html>
